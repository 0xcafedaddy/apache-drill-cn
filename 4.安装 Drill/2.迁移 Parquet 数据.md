# 迁移 Parquet 数据

你使用的 Drill 1.2 或更早的版本，是需要强制迁移 Parquet 数据的，让其兼容最新的版本。数据必需被标记为是由 Drill 生成的。

  ```
  重要：运行升级工具只需要作用于 Drill 生成的 Parquet 文件上。
  ```

## 为什么迁移 Drill 数据

Drill 1.3 之后采用了 Apache Parquet 库去生成和分解 Parquet 文件，而 Drill 1.2 和早期使用的是它自己的一个 Parquet 库。Drill 团队创建了自己的版本来修复旧的库，让其能够准确的处理其他工具生成的 Parquet 文件，例如 Impala 和 Hive。Apache Parquet 在最新版本修复了这个 BUG，使它适用于 Drill 1.3 和之后的版本。Drill 现在使用的是 Apache Parquet 库用于 Impala，Hive 和其他软件。你需要运行升级工具去升级 Drill 1.2 和更早版本的库文件。

升级工具只需要简单的插入一个版本号到元数据，用以标记文件是 Drill 文件。

## 准备迁移

留出足够的时间进行迁移。在 Drill 的开发测试中，花了 32 分钟去升级 1TB 数据，其中包含 840 个文件；另外，花了 370 分钟去升级 100GB 数据，其中包含 200000 个文件。虽然文件大小是影响升级花费时间的因素，但是文件数量是最重要的因素。

系统管理员可以写一个脚本来运行升级工具到多个子目录。

备份数据，在迁移数据之前，创建一个或多个临时目录用于描述下一部分的内容。

## 如何迁移数据

在修改系统故障的情况下，在临时目录或目录中保存一份文件的副本。检查临时目录也可以表明数据是否迁移成功。

迁移 Parquet 数据需在 Drill 1.3 或之后的版本，步骤如下所示：
```
重要：运行升级工具只需要作用于 Drill 生成的 Parquet 文件上。
```
  1. 在迁移之前备份数据。
  2. 在同一个文件系统中作为数据，创建一个或多个临时目录，这取决你如何去执行你的升级工具。例如，你的数据在 HDFS，创建一个临时目录到 HDFS 上。创建不同的临时目录，当你在多个目录同时运行升级工具时，因为不同的目录可以有相同名称的文件。
  3. 从 [Github](https://github.com/parthchandra/drill-upgrade) 上下载并编译升级工具。
  4. 如果你使用 Parquet 元数据缓存：
    - 当你计划运行升级工具时，从所有的目录和子目录里删除你生成的缓存文件。
    - 当缓存文件事先存在，你需要运行 ``` REFRESH TABLE METADATA ``` 在所有的文件夹上。
  5. 使用以下示例去运行升级工具：
    ```bash
    java -Dlog.path=/<your path>/drill-upgrade/upgrade.log -cp drill-upgrade-1.0-jar-with-dependencies.jar org.apache.drill.upgrade.Upgrade_12_13 --tempDir=maprfs:///drill/upgrade-temp maprfs:///drill/testdata/
    ```

## 检查迁移是否成功

如果你在无人监控的情况执行迁移，检查你的临时目录或目录是否为空。空目录表示成功。

## 处理迁移失败

如果网络连接挂掉，或者用户取消操作，在取消时被处理的文件可能已损坏。在取消时被处理的文件可能已损坏：
  1. 拷贝临时文件到你的 Parquet 文件目录。
  2. 重新运行升级工具。

工具会跳过已处理的文件，只更新剩余的文件。
